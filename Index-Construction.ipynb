{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "import bisect\n",
    "import pygtrie\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "folder_path=\"C:\\\\Users\\\\Archana\\\\Documents\\\\SEM_7\\\\3_AIR\\\\TelevisionNews\"\n",
    "files= os.listdir(folder_path)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['URL', 'MatchDateTime', 'Station', 'Show', 'IAShowID', 'IAPreviewThumb',\n",
      "       'Snippet'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "file0=folder_path+\"\\\\\"+files[0]\n",
    "df=pd.read_csv(file0)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopword(text):    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    l=[\"not\",\"no\", \"up\",\"down\",\"under\",\"above\",\"below\",\"own\",\"on\",\"off\",\"out\",\"through\",\"won\",\"against\",\"now\",\"before\",\"after\"]    \n",
    "    # retaining some stopwords\n",
    "    for word in l:\n",
    "        stop_words.remove(word)    \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    return(\" \".join(filtered_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to remove special characters and punctuations\n",
    "def clean_text(text):\n",
    "    # decimals? \n",
    "    text=text.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "    punclist='!\"#$%&()*+,-./:;<=>?@[]\\^_{}|~'\n",
    "    t=str.maketrans(dict.fromkeys(punclist, \" \"))\n",
    "    text=text.translate(t)\n",
    "    t=str.maketrans(dict.fromkeys(\"'`\",\"\"))\n",
    "    text=text.translate(t)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "def my_lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text=text.split()\n",
    "    l=[]\n",
    "    for word in text:\n",
    "        lword=lemmatizer.lemmatize(word)\n",
    "        l.append(lword)\n",
    "    \n",
    "    return \" \".join(l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrapper function, called for each file\n",
    "# returns a dataframe with new preprocessed column \"Text\" \n",
    "def pre_process(file_path,col=\"Snippet\"):\n",
    "    df=pd.read_csv(file_path)\n",
    "    l=[]\n",
    "    column=df[col]\n",
    "    for row in column:\n",
    "        l.append(my_lemmatize(remove_stopword(clean_text(row))))\n",
    "    df[\"Text\"]=l\n",
    "    \n",
    "    return df\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# updating positional index\n",
    "def update_trie(term,docid,pos,trie):\n",
    "    if term in trie:\n",
    "        if docid in trie[term]:\n",
    "            bisect.insort(trie[term][docid],pos) #insert into sorted list of positions\n",
    "        else:\n",
    "            trie[term][docid]=[pos]\n",
    "    else:\n",
    "        trie[term]={docid:[pos]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to create index for each file\n",
    "# returns (normal index, reverse index)\n",
    "# trie node: key,value pairs\n",
    "#            key - <term>, value- {docId1: [pos1, pos2, pos3...], docId2: [pos1,pos2...]}\n",
    "def index_construction(file_path):\n",
    "    df=pre_process(file_path)\n",
    "    corpus=df[\"Text\"]\n",
    "\n",
    "    #creating 2 tries\n",
    "    index_trie=pygtrie.CharTrie()\n",
    "    rev_trie=pygtrie.CharTrie()\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        row=word_tokenize(corpus[i])\n",
    "        for j in range(len(row)):\n",
    "            update_trie(row[j],i,j,index_trie)            \n",
    "            update_trie(row[j][::-1],i,j,rev_trie)\n",
    "    \n",
    "    return (index_trie,rev_trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating index for one file\n",
    "index_trie=index_construction(folder_path+\"\\\\\"+files[0])\n",
    "#print(index_trie[0]) # normal index\n",
    "#print(index_trie[1]) # reverse index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN.200910.csv\n"
     ]
    }
   ],
   "source": [
    "# creating index for each file\n",
    "all_indexes=[]\n",
    "for i in files:\n",
    "    try:\n",
    "        all_indexes.append(index_construction(folder_path+\"\\\\\"+i))\n",
    "    except:\n",
    "        print(i) # file CNN.200910.csv is empty in the dataset\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n"
     ]
    }
   ],
   "source": [
    "print(len(all_indexes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
